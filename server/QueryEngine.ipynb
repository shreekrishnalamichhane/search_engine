{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aec54600",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import string \n",
    "import numpy as np \n",
    "import nltk\n",
    "import bson\n",
    "import pymongo as pm\n",
    "from itertools import chain\n",
    "import math \n",
    "import asyncio\n",
    "from bson.objectid import ObjectId\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer, WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c4c05dd",
   "metadata": {},
   "source": [
    "# Initialize PyMongo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b370afe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database(MongoClient(host=['localhost:27017'], document_class=dict, tz_aware=False, connect=True, authsource='admin'), 'db_seven_sem_prj')\n"
     ]
    }
   ],
   "source": [
    "mongouri = \"mongodb://root:prisma@localhost:27017/db_seven_sem_prj?authSource=admin\"\n",
    "client = pm.MongoClient(mongouri)\n",
    "database = client.get_database()\n",
    "print(database)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9932594",
   "metadata": {},
   "source": [
    "# Save Collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f14f895",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_collection = database[\"tokens\"]\n",
    "website_collection = database[\"websites\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b06929e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5277\n"
     ]
    }
   ],
   "source": [
    "total_websites = len(list(website_collection.find()))\n",
    "\n",
    "x = website_collection.count_documents({})\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf4a3dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def textPreProcessing(text):\n",
    "    nepaliStopWords = [\"अझै\",\"अधिक\",\"अन्य\",\"अन्यत्र\",\"अन्यथा\",\"अब\",\"अरु\",\"अरुलाई\",\"अर्को\",\"अर्थात\",\"अर्थात्\",\"अलग\",\"आए\",\"आजको\",\"आत्म\",\"आदि\",\"आफू\",\"आफूलाई\",\"आफै\",\"आफैलाई\",\"आफैले\",\"आफ्नै\",\"आफ्नो\",\"आयो\",\"उनको\",\"उनले\",\"उनि\",\"उनी\",\"उनीहरु\",\"उप\",\"उसलाई\",\"उस्तै\",\"उहाँ\",\"उहालाई\",\"ऊ\",\"एउटै\",\"एक\",\"एकदम\",\"ओठ\",\"औं\",\"कतै\",\"कसरी\",\"कसै\",\"कसैले\",\"कस्तो\",\"कहाँ\",\"कहाँबाट\",\"कहिले\",\"कहिलेकाहीं\",\"का\",\"कि\",\"किन\",\"किनभने\",\"कुनै\",\"कुरा\",\"कृपया\",\"के\",\"केवल\",\"केहि\",\"केही\",\"को\",\"कोही\",\"गए\",\"गयौ\",\"गर\",\"गरि\",\"गरी\",\"गरे\",\"गरेका\",\"गरेको\",\"गरेर\",\"गरौं\",\"गर्छ\",\"गर्छु\",\"गर्दछ\",\"गर्दै\",\"गर्न\",\"गर्नु\",\"गर्नुपर्छ\",\"गर्ने\",\"गर्नेछन्\",\"गर्नेछौ\",\"गैर\",\"चार\",\"चाले\",\"चाहनुहुन्छ\",\"चाहन्छु\",\"चाहन्छौ\",\"चाहन्छौं\",\"चाहन्थे\",\"चाहिए\",\"छ\",\"छन्\",\"छु\",\"छू\",\"छैन\",\"छौं\",\"जब\",\"जबकि\",\"जसको\",\"जसबाट\",\"जसमा\",\"जसलाई\",\"जसले\",\"जस्तै\",\"जस्तो\",\"जहाँ\",\"जान\",\"जाहिर\",\"जुन\",\"जे\",\"जो\",\"ठीक\",\"त\",\"तत्काल\",\"तथा\",\"तदनुसार\",\"तपाई\",\"तपाईं\",\"तपाईको\",\"तर\",\"तल\",\"तापनी\",\"तिनिहरुलाई\",\"तिनी\",\"तिनीहरुको\",\"तिनीहरू\",\"तिनीहरूको\",\"तिमि\",\"तिमी\",\"तिमीसँग\",\"तिम्रो\",\"तिर\",\"ती\",\"तीन\",\"तुरुन्तै\",\"तेस्कारण\",\"तेस्रो\",\"त्यसपछि\",\"त्यहाँ\",\"त्यो\",\"त्सपछि\",\"त्सैले\",\"थप\",\"थिए\",\"थिएन\",\"थिएनन्\",\"थियो\",\"दिए\",\"दिनुभएको\",\"दिनुहुन्छ\",\"दुई\",\"दुबै\",\"देखि\",\"देखिन्छ\",\"देखियो\",\"देखे\",\"देखेको\",\"देखेर\",\"दोस्रो\",\"द्वारा\",\"धेरै\",\"न\",\"नगर्नुहोस्\",\"नजिकै\",\"नत्र\",\"नयाँ\",\"नि\",\"निम्ति\",\"निम्न\",\"नै\",\"नौ\",\"पक्का\",\"पक्कै\",\"पछि\",\"पछिल्लो\",\"पटक\",\"पनि\",\"पर्छ\",\"पर्थ्यो\",\"पर्याप्त\",\"पहिले\",\"पहिलो\",\"पहिल्यै\",\"पाँच\",\"पाँचौं\",\"पूर्व\",\"प्रति\",\"प्रतेक\",\"प्रत्येक\",\"प्लस\",\"फेरि\",\"फेरी\",\"बने\",\"बन्द\",\"बरु\",\"बाट\",\"बारे\",\"बारेमा\",\"बाहिर\",\"बाहेक\",\"बिरुद्ध\",\"बिशेष\",\"बीच\",\"बीचमा\",\"भए\",\"भएको\",\"भन\",\"भने\",\"भन्\",\"भन्छन्\",\"भन्छु\",\"भन्दा\",\"भन्नुभयो\",\"भन्ने\",\"भर\",\"भित्र\",\"भित्री\",\"म\",\"मँ\",\"मलाई\",\"मा\",\"मात्र\",\"माथि\",\"मार्फत\",\"मुख्य\",\"मेरो\",\"मैले\",\"यति\",\"यथोचित\",\"यदि\",\"यद्यपि\",\"यस\",\"यसको\",\"यसपछि\",\"यसबाहेक\",\"यसरी\",\"यसैले\",\"यसो\",\"यस्तो\",\"यहाँ\",\"यहाँसम्म\",\"या\",\"यी\",\"यो\",\"र\",\"रही\",\"रहेका\",\"रहेको\",\"राखे\",\"राख्छ\",\"राम्रो\",\"रूप\",\"लगभग\",\"लाई\",\"लागि\",\"ले\",\"वरीपरी\",\"वा\",\"वास्तवमा\",\"विरुद्ध\",\"शायद\",\"सकदिन\",\"सकिएन\",\"सक्छ\",\"सक्दैन\",\"संग\",\"संगै\",\"सट्टा\",\"सधै\",\"सबै\",\"सबैलाई\",\"समय\",\"समयमा\",\"सम्भव\",\"सम्म\",\"सही\",\"साँच्चै\",\"सात\",\"साथ\",\"साथै\",\"सायद\",\"सारा\",\"सो\",\"सोही\",\"स्पष्ट\",\"हरे\",\"हरेक\",\"हामी\",\"हामीसँग\",\"हाम्रो\",\"हुँ\",\"हुँदैन\",\"हुन\",\"हुनु\",\"हुनुहुन्छ\",\"हुने\",\"हुनेछ\",\"हुनेछु\",\"हुन्\",\"हुन्छ\",\"हुन्थे\",\"हो\",\"होइन\",\"हौंअझै\",\"अधिक\",\"अन्य\",\"अन्यत्र\",\"अन्यथा\",\"अब\",\"अरु\",\"अरुलाई\",\"अर्को\",\"अर्थात\",\"अर्थात्\",\"अलग\",\"आए\",\"आजको\",\"आत्म\",\"आदि\",\"आफू\",\"आफूलाई\",\"आफै\",\"आफैलाई\",\"आफैले\",\"आफ्नै\",\"आफ्नो\",\"आयो\",\"उनको\",\"उनले\",\"उनि\",\"उनी\",\"उनीहरु\",\"उप\",\"उसलाई\",\"उस्तै\",\"उहाँ\",\"उहालाई\",\"ऊ\",\"एउटै\",\"एक\",\"एकदम\",\"ओठ\",\"औं\",\"कतै\",\"कसरी\",\"कसै\",\"कसैले\",\"कस्तो\",\"कहाँ\",\"कहाँबाट\",\"कहिले\",\"कहिलेकाहीं\",\"का\",\"कि\",\"किन\",\"किनभने\",\"कुनै\",\"कुरा\",\"कृपया\",\"के\",\"केवल\",\"केहि\",\"केही\",\"को\",\"कोही\",\"गए\",\"गयौ\",\"गर\",\"गरि\",\"गरी\",\"गरे\",\"गरेका\",\"गरेको\",\"गरेर\",\"गरौं\",\"गर्छ\",\"गर्छु\",\"गर्दछ\",\"गर्दै\",\"गर्न\",\"गर्नु\",\"गर्नुपर्छ\",\"गर्ने\",\"गर्नेछन्\",\"गर्नेछौ\",\"गैर\",\"चार\",\"चाले\",\"चाहनुहुन्छ\",\"चाहन्छु\",\"चाहन्छौ\",\"चाहन्छौं\",\"चाहन्थे\",\"चाहिए\",\"छ\",\"छन्\",\"छु\",\"छू\",\"छैन\",\"छौं\",\"जब\",\"जबकि\",\"जसको\",\"जसबाट\",\"जसमा\",\"जसलाई\",\"जसले\",\"जस्तै\",\"जस्तो\",\"जहाँ\",\"जान\",\"जाहिर\",\"जुन\",\"जे\",\"जो\",\"ठीक\",\"त\",\"तत्काल\",\"तथा\",\"तदनुसार\",\"तपाई\",\"तपाईं\",\"तपाईको\",\"तर\",\"तल\",\"तापनी\",\"तिनिहरुलाई\",\"तिनी\",\"तिनीहरुको\",\"तिनीहरू\",\"तिनीहरूको\",\"तिमि\",\"तिमी\",\"तिमीसँग\",\"तिम्रो\",\"तिर\",\"ती\",\"तीन\",\"तुरुन्तै\",\"तेस्कारण\",\"तेस्रो\",\"त्यसपछि\",\"त्यहाँ\",\"त्यो\",\"त्सपछि\",\"त्सैले\",\"थप\",\"थिए\",\"थिएन\",\"थिएनन्\",\"थियो\",\"दिए\",\"दिनुभएको\",\"दिनुहुन्छ\",\"दुई\",\"दुबै\",\"देखि\",\"देखिन्छ\",\"देखियो\",\"देखे\",\"देखेको\",\"देखेर\",\"दोस्रो\",\"द्वारा\",\"धेरै\",\"न\",\"नगर्नुहोस्\",\"नजिकै\",\"नत्र\",\"नयाँ\",\"नि\",\"निम्ति\",\"निम्न\",\"नै\",\"नौ\",\"पक्का\",\"पक्कै\",\"पछि\",\"पछिल्लो\",\"पटक\",\"पनि\",\"पर्छ\",\"पर्थ्यो\",\"पर्याप्त\",\"पहिले\",\"पहिलो\",\"पहिल्यै\",\"पाँच\",\"पाँचौं\",\"पूर्व\",\"प्रति\",\"प्रतेक\",\"प्रत्येक\",\"प्लस\",\"फेरि\",\"फेरी\",\"बने\",\"बन्द\",\"बरु\",\"बाट\",\"बारे\",\"बारेमा\",\"बाहिर\",\"बाहेक\",\"बिरुद्ध\",\"बिशेष\",\"बीच\",\"बीचमा\",\"भए\",\"भएको\",\"भन\",\"भने\",\"भन्\",\"भन्छन्\",\"भन्छु\",\"भन्दा\",\"भन्नुभयो\",\"भन्ने\",\"भर\",\"भित्र\",\"भित्री\",\"म\",\"मँ\",\"मलाई\",\"मा\",\"मात्र\",\"माथि\",\"मार्फत\",\"मुख्य\",\"मेरो\",\"मैले\",\"यति\",\"यथोचित\",\"यदि\",\"यद्यपि\",\"यस\",\"यसको\",\"यसपछि\",\"यसबाहेक\",\"यसरी\",\"यसैले\",\"यसो\",\"यस्तो\",\"यहाँ\",\"यहाँसम्म\",\"या\",\"यी\",\"यो\",\"र\",\"रही\",\"रहेका\",\"रहेको\",\"राखे\",\"राख्छ\",\"राम्रो\",\"रूप\",\"लगभग\",\"लाई\",\"लागि\",\"ले\",\"वरीपरी\",\"वा\",\"वास्तवमा\",\"विरुद्ध\",\"शायद\",\"सकदिन\",\"सकिएन\",\"सक्छ\",\"सक्दैन\",\"संग\",\"संगै\",\"सट्टा\",\"सधै\",\"सबै\",\"सबैलाई\",\"समय\",\"समयमा\",\"सम्भव\",\"सम्म\",\"सही\",\"साँच्चै\",\"सात\",\"साथ\",\"साथै\",\"सायद\",\"सारा\",\"सो\",\"सोही\",\"स्पष्ट\",\"हरे\",\"हरेक\",\"हामी\",\"हामीसँग\",\"हाम्रो\",\"हुँ\",\"हुँदैन\",\"हुन\",\"हुनु\",\"हुनुहुन्छ\",\"हुने\",\"हुनेछ\",\"हुनेछु\",\"हुन्\",\"हुन्छ\",\"हुन्थे\",\"हो\",\"होइन\",\"हौं\"]\n",
    "    punctuation = string.punctuation + \"।\" + \"!\" + \"?\"\n",
    "    stopWordsEn = set(stopwords.words(\"english\"))\n",
    "    stopWordsNp = set(nepaliStopWords)\n",
    "    stopWordsCombined = stopWordsEn.union(stopWordsNp)\n",
    "    \n",
    "    # Remove Punctuations\n",
    "    text = text.translate(str.maketrans(\"\", \"\", punctuation))\n",
    "    # Tokenize Word     \n",
    "    tokens = word_tokenize(text)\n",
    "    # Remove Stop Words     \n",
    "    tokens = [token for token in tokens if token.lower() not in stopWordsCombined]\n",
    "    \n",
    "    # Stemming     \n",
    "    stemmerEn = SnowballStemmer(\"english\")\n",
    "    tokens = [stemmerEn.stem(token) for token in tokens]\n",
    "    \n",
    "    # Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    \n",
    "    tokens_occ = []\n",
    "    \n",
    "    for token in set(tokens):\n",
    "        occurances = [index for index, value in enumerate(tokens) if value == token]\n",
    "        tokens_occ.append({\"token\": token, \"occurrences\": occurances})\n",
    "    return tokens_occ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5ed5a96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"kp oli\"\n",
    "tokenized_query = textPreProcessing(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "821d4856",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([{'token': 'oli', 'occurrences': [1]}, {'token': 'kp', 'occurrences': [0]}],\n",
       " ['oli', 'kp'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_query\n",
    "extracted_tokens = [t[\"token\"] for t in tokenized_query]\n",
    "tokenized_query, extracted_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d9303f",
   "metadata": {},
   "source": [
    "# Assign TF, IDF Score to each document_token combo to create a document matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "69b459fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all websites where each tokens occur\n",
    "pipeline = [\n",
    "    {\n",
    "        \"$match\": {\n",
    "            \"token\": {\"$in\": extracted_tokens}\n",
    "        }\n",
    "    },\n",
    "]\n",
    "token_db_cursor = token_collection.aggregate(pipeline)\n",
    "token_db_list = list(token_db_cursor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "67a60ddc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idf_table = {} #table where keys are website_ids and value is dict of tokens whose value is tf_idf = tdf * idf\n",
    "total_docs_token_is_in = {}\n",
    "for token in token_db_list:\n",
    "    total_docs_token_is_in[token[\"token\"]] = len(token[\"websites\"])\n",
    "    for website in token[\"websites\"]:\n",
    "        table_key = str(website[\"_id\"])\n",
    "        occurrences = website[\"occurrences\"]\n",
    "        if table_key not in idf_table:\n",
    "            idf_table[table_key] = {}\n",
    "            idf_table[table_key][token[\"token\"]] = {\"occurrences\": occurrences}              \n",
    "        else:\n",
    "            idf_table[table_key][token[\"token\"]] = {\"occurrences\": occurrences}              \n",
    "\n",
    "            \n",
    "            \n",
    "website_ids = list([ObjectId(k) for k in idf_table.keys()])\n",
    "websites = list(website_collection.find({\"_id\": {\"$in\": website_ids}}, {\"total_tokens\":1, \"_id\": 1}))\n",
    "websites_token_cnt = [{\"_id\": str(ws[\"_id\"]), \"token_len\": len(ws[\"total_tokens\"])} for ws in websites]\n",
    "\n",
    "document_vector = {}\n",
    "token_idf = {} #needed for query vector\n",
    "\n",
    "for idx, ws in enumerate(websites_token_cnt):\n",
    "    idf_table_tokens = idf_table[ws[\"_id\"]]\n",
    "    for qToken in extracted_tokens:\n",
    "        if qToken not in idf_table_tokens.keys():\n",
    "            idf_table_tokens[qToken] = {\"occurrences\": []}\n",
    "            \n",
    "    for token in idf_table_tokens:\n",
    "        curr_token_freq_in_ws = len(idf_table_tokens[token][\"occurrences\"])\n",
    "        total_tokens_in_ws =  ws[\"token_len\"]\n",
    "        \n",
    "        \n",
    "        \n",
    "        token_ws_tf = curr_token_freq_in_ws / total_tokens_in_ws\n",
    "        token_ws_idf = math.log(total_websites / total_docs_token_is_in[token])\n",
    "        token_ws_tfidf = token_ws_tf * token_ws_idf\n",
    "        \n",
    "        token_idf[token] = token_ws_idf\n",
    "        \n",
    "        idf_table_tokens[token][\"tfidf\"] = token_ws_tfidf\n",
    "        idf_table_tokens[token][\"idf\"] = token_ws_idf\n",
    "        \n",
    "    idf_table[ws[\"_id\"]] = idf_table_tokens\n",
    "\n",
    "    \n",
    "for ws in idf_table:\n",
    "    table_entry = idf_table[ws]\n",
    "    if ws not in document_vector:\n",
    "        document_vector[ws] = [-1] * len(extracted_tokens)\n",
    "        for idx, qToken in enumerate(extracted_tokens): #this ensures order for token tfidf of document matrix is same as order of tokens in query\n",
    "            document_vector[ws][idx] = table_entry[qToken][\"tfidf\"]\n",
    "    else:\n",
    "        for idx, qToken in enumerate(extracted_tokens): #this ensures order for token tfidf of document matrix is same as order of tokens in query\n",
    "            document_vector[ws][idx] = table_entry[qToken][\"tfidf\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e579f61",
   "metadata": {},
   "source": [
    "# Create Query Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "80dacffd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "query_vector = []\n",
    "\n",
    "for qToken in tokenized_query:\n",
    "    qToken_tf = len(qToken[\"occurrences\"]) / len(tokenized_query)\n",
    "    if(qToken[\"token\"] in token_idf):\n",
    "        qToken_idf = token_idf[qToken[\"token\"]]\n",
    "        qToken_tf_idf = qToken_tf * qToken_idf\n",
    "        query_vector.append(qToken_tf_idf)\n",
    "\n",
    "print(query_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b52fc7ca",
   "metadata": {},
   "source": [
    "# Calculate Cosine SImilarity Between Query Vector and all Document Vectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "01d4bfa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "cosine_similarity = {}\n",
    "for doc in document_vector:\n",
    "    doc_vec_arr = np.array(document_vector[doc])\n",
    "    query_vec_arr = np.array(query_vector)\n",
    "    cosSim = np.dot(doc_vec_arr, query_vec_arr)\n",
    "    cosine_similarity[doc] = cosSim\n",
    "\n",
    "print(len(cosine_similarity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "37f7a2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_ws_score_list = [_id for _id in cosine_similarity]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ffdba777",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ws_for_query = website_collection.aggregate([\n",
    "    {\n",
    "        \"$match\":{\n",
    "            \"_id\": {\n",
    "                \"$in\": [ObjectId(_id) for _id in final_ws_score_list]\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"$project\":{\n",
    "            \"_id\": 1,\n",
    "            \"url\": 1,\n",
    "            \"rank\":1,\n",
    "            \"total_tokens\":1\n",
    "        }\n",
    "    }\n",
    "])\n",
    "ws_for_query_list = list(ws_for_query)\n",
    "ws_for_query_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b617f714",
   "metadata": {},
   "source": [
    "# Use Weighted Sum for Determining the rank\n",
    "Using 20% for Website rank from page rank and 80% from Cosine Similarity. Could use Gradient Decent for better calculation results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "10aef12f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "final_rank_score = {}\n",
    "\n",
    "COSINE_RANK_WT = 0.8\n",
    "PAGE_RANK_WT = 0.2\n",
    "\n",
    "for ws in ws_for_query_list:\n",
    "    _id = str(ws[\"_id\"])\n",
    "    page_rank = ws[\"rank\"]\n",
    "    cosine_rank = cosine_similarity[_id]\n",
    "    final_rank = COSINE_RANK_WT * cosine_rank + PAGE_RANK_WT * page_rank\n",
    "    final_rank_score[_id] = final_rank\n",
    "    \n",
    "print(len(final_rank_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "19d5c173",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort document_ids by cosine similarity valyr\n",
    "sorted_ws = sorted(ws_for_query_list, key=lambda x: final_rank_score.get(str(x[\"_id\"]), 0), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4dd1b6ea",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "max() arg is an empty sequence",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_274043/2068258063.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Test if the item on top is actually the item with highest score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mhighest_value_in_final_rank\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinal_rank_score\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0m_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msorted_ws\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"_id\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0misMatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhighest_value_in_final_rank\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: max() arg is an empty sequence"
     ]
    }
   ],
   "source": [
    "    # Test if the item on top is actually the item with highest score\n",
    "highest_value_in_final_rank = max(final_rank_score.items(), key=lambda x: x[1])\n",
    "\n",
    "_id = str(sorted_ws[0][\"_id\"])\n",
    "isMatch = highest_value_in_final_rank[0] == _id\n",
    "\"PASS\" if isMatch == True else \"Fail\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1bab80ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_ws"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
